{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njainds/Colab_notebooks/blob/master/CCP_ICD10_Supmodel_v2/M_maxlen7_350_icd/CNNmodelv2_v6.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYpecTXf_H6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Builf train and evaluate\n",
        "# Save model for offline scoring on test\n",
        "# Model name: CCP_ICD10_Searchengine/CNNmodel_v1.0.ipynb\n",
        "# References\n",
        "#https://github.com/njainds/Colab_notebooks/blob/master/Kaggle_Toxic_Comments/Model-2-keras_lstmConv.ipynb\n",
        "#https://github.com/njainds/NLP/blob/master/kaggle/QIQC/22nd%20Place%20Solution%20-%206%20models%20%2B%20POS%20tagging.ipynb\n",
        "#https://github.com/Cheneng/DPCNN/blob/master/model/DPCNN.py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AL5rrZqLghp",
        "colab_type": "code",
        "outputId": "aed6d4fc-4527-43ca-c06b-51d1ca58896c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "print(\"Name of GPU : {}\".format(torch.cuda.get_device_name(0)))\n",
        "print(\"# of GPU : {}\".format(torch.cuda.device_count()))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of GPU : Tesla T4\n",
            "# of GPU : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zRdXN9yo6k3",
        "colab_type": "code",
        "outputId": "12a20d69-aa93-4393-f952-04e6e59c7479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas(desc='Progress')\n",
        "\n",
        "import os\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import Input, Dense, Embedding, concatenate, CuDNNGRU, CuDNNLSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Lambda, Permute, Reshape, merge, Dropout, Conv2D, MaxPool2D, Concatenate, Conv1D, MaxPool1D, add, MaxPooling1D\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D, Add\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from keras import callbacks\n",
        "\n",
        "# cross validation and metrics\n",
        "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection,linear_model,metrics\n",
        "from sklearn.metrics import roc_auc_score,accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhSwmpxVpCSx",
        "colab_type": "code",
        "outputId": "6b1b60c4-ff2b-4182-db3a-ba8cd2798b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#!ls \"/content/drive/My Drive/CCP-ICDsearch/desc-icd9-modeldata_v2\"\n",
        "os.chdir(\"/content/drive/My Drive/CCP-ICDsearch/desc-icd9-modeldata_v2/d_maxlen7_350_icd\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiHWhcY9SszU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.load(\"train_X.npy\")\n",
        "x_test = np.load(\"test_X.npy\")\n",
        "y_train = np.load(\"train_y.npy\")\n",
        "y_test = np.load(\"test_y.npy\")\n",
        "features = np.load(\"test_y.npy\")\n",
        "icd_dict = np.load(\"icd_dict.npy\").item()\n",
        "word_index = np.load(\"word_index.npy\").item()\n",
        "embeddings = np.load(\"embedding_matrix.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nTOOQtvDHeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CyclicLR(Callback):\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=4590., mode='triangular',gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "    \n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size,\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwjoGzOR3m9H",
        "colab_type": "code",
        "outputId": "0365c5b8-d737-4fe7-a623-c4a31295bef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class RocAucEvaluation(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
        "\n",
        "#del check_point, ra_val, early_stop\n",
        "!rm ./best_model.hdf5\n",
        "file_path = \"best_model.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
        "ra_val = RocAucEvaluation(validation_data=(x_test, y_test), interval = 1)\n",
        "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './best_model.hdf5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5xLwVe9Bw91",
        "colab_type": "code",
        "outputId": "367731ff-aaea-4dcd-e08d-673935558cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#configs\n",
        "os.chdir(\"/content/drive/My Drive/CCP-ICDsearch/desc-icd9-modeldata_v2/M_maxlen7_350_icd\")\n",
        "max_features = embeddings.shape[0]\n",
        "maxlen = x_train.shape[1]\n",
        "embed_size = embeddings.shape[1]\n",
        "n_class = y_train.shape[1]\n",
        "print(max_features,maxlen, embed_size, n_class)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13735 7 200 350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-tXdS0MM5aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dpcnn_model(num_block = 5, k = 3, units = 128):\n",
        "    pad = 'same'\n",
        "    keras.backend.clear_session()\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    embedding_layer1   = Embedding(max_features, embed_size, weights=[embeddings], trainable=True)(inp)\n",
        "    embedding_layer1 = SpatialDropout1D(0.2)(embedding_layer1)\n",
        "    emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(embedding_layer1)\n",
        "    emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(emb_short_cut)\n",
        "    # Main block\n",
        "    for b in range(1, num_block + 1):\n",
        "        if b == 1:\n",
        "            block = embedding_layer1\n",
        "            short_cut = emb_short_cut\n",
        "        else:\n",
        "            block = block\n",
        "            short_cut = block\n",
        "        block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
        "        block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
        "        block = add([short_cut, block])\n",
        "        block = MaxPooling1D(pool_size = 3, strides = 2, padding = pad)(block)\n",
        "    # Final block\n",
        "    short_cut = block\n",
        "    block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
        "    block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(block)\n",
        "    block = add([short_cut, block])\n",
        "    max_pool = GlobalMaxPooling1D()(block)\n",
        "    avg_pool = GlobalAveragePooling1D()(block)\n",
        "    last = (Lambda(lambda x: x[:,-1,:]) (block))\n",
        "    block = concatenate([max_pool, avg_pool, last])\n",
        "    #reverse\n",
        "    rev_embedding_layer = Lambda(lambda x: K.reverse(x,axes=-1))(embedding_layer1)\n",
        "    rev_emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(rev_embedding_layer)\n",
        "    rev_emb_short_cut = Conv1D(units, kernel_size = 1, padding = pad, activation = 'relu')(rev_emb_short_cut)\n",
        "    # Main block\n",
        "    for b in range(1, num_block + 1):\n",
        "        if b == 1:\n",
        "            rev_block = rev_embedding_layer\n",
        "            rev_short_cut = rev_emb_short_cut\n",
        "        else:\n",
        "            rev_block = rev_block\n",
        "            rev_short_cut = rev_block\n",
        "        rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
        "        rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
        "        rev_block = add([rev_short_cut, rev_block])\n",
        "        rev_block = MaxPooling1D(pool_size = 3, strides = 2, padding = pad)(rev_block)\n",
        "    # Final block\n",
        "    rev_short_cut = rev_block\n",
        "    rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
        "    rev_block = Conv1D(units, kernel_size = k, padding = pad, activation = 'relu')(rev_block)\n",
        "    rev_block = add([rev_short_cut, rev_block])\n",
        "    rev_max_pool = GlobalMaxPooling1D()(rev_block)\n",
        "    rev_avg_pool = GlobalAveragePooling1D()(rev_block)\n",
        "    rev_last = (Lambda(lambda x: x[:,-1,:]) (rev_block))\n",
        "    rev_block = concatenate([rev_max_pool, rev_avg_pool, rev_last])\n",
        "    block = concatenate([rev_block, block])\n",
        "    # output block\n",
        "    out_put = Dense(128, activation = 'relu')(block)\n",
        "    outp = Dense(n_class, activation='softmax')(out_put)\n",
        "    model=Model(inputs=inp,outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rej4RS8O1JF",
        "colab_type": "code",
        "outputId": "3d385eb7-6f17-4c0e-da81-d5cc38e01d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5681
        }
      },
      "source": [
        "# Kfold training of model\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, random_state=2019, shuffle=True)\n",
        "pred_test = np.zeros((x_test.shape[0], n_class))\n",
        "\n",
        "for i, (train_index, valid_index) in enumerate(kfold.split(x_train, np.argmax(y_train, axis=1))):\n",
        "    X_train, X_val, Y_train, Y_val = x_train[train_index], x_train[valid_index], y_train[train_index], y_train[valid_index]\n",
        "    \n",
        "    file_path = \"best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
        "    ra_val = RocAucEvaluation(validation_data=(X_val, Y_val), interval = 1)\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
        "    \n",
        "    model = build_dpcnn_model(num_block = 5, k = 3, units = 128)\n",
        "    \n",
        "    print(\"### Model with seed: {}  for fold no. {} ####\".format( 2019, i))\n",
        "    \n",
        "    clr = CyclicLR(base_lr=1e-6, max_lr=1e-3,step_size=5420., mode='exp_range',gamma=0.99994)\n",
        "    model.fit(X_train, Y_train, batch_size = 48, epochs = 10, validation_data = (X_val, Y_val), verbose = 1, callbacks = [clr, ra_val, check_point, early_stop])\n",
        "    \n",
        "    model_json = model.to_json()\n",
        "    with open(\"model%s.json\" % i, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"model%s.h5\" %i)\n",
        "    print(\"Saved model to disk\")\n",
        "    \n",
        "    pred_train = model.predict([X_val], batch_size=48, verbose=2)\n",
        "    pred_test += model.predict([x_test], batch_size=48, verbose=1)/5\n",
        "    os.remove(file_path)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "### Model with seed: 2019  for fold no. 0 ####\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 41496 samples, validate on 10537 samples\n",
            "Epoch 1/10\n",
            "41496/41496 [==============================] - 22s 518us/step - loss: 4.9489 - acc: 0.1391 - val_loss: 3.5515 - val_acc: 0.3274\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.864931\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.55147, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "41496/41496 [==============================] - 17s 421us/step - loss: 2.8177 - acc: 0.4218 - val_loss: 2.1653 - val_acc: 0.5335\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.967357\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.55147 to 2.16532, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "41496/41496 [==============================] - 18s 442us/step - loss: 1.9753 - acc: 0.5628 - val_loss: 1.7636 - val_acc: 0.6108\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.978802\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.16532 to 1.76365, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "41496/41496 [==============================] - 17s 419us/step - loss: 1.6111 - acc: 0.6341 - val_loss: 1.5617 - val_acc: 0.6497\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.983117\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.76365 to 1.56168, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "41496/41496 [==============================] - 18s 444us/step - loss: 1.3877 - acc: 0.6792 - val_loss: 1.4478 - val_acc: 0.6849\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.984332\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.56168 to 1.44783, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "41496/41496 [==============================] - 17s 420us/step - loss: 1.2436 - acc: 0.7089 - val_loss: 1.4050 - val_acc: 0.6992\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.985191\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.44783 to 1.40504, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "41496/41496 [==============================] - 19s 448us/step - loss: 1.1180 - acc: 0.7317 - val_loss: 1.3542 - val_acc: 0.7115\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.985651\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.40504 to 1.35417, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "41496/41496 [==============================] - 18s 423us/step - loss: 0.9619 - acc: 0.7644 - val_loss: 1.3450 - val_acc: 0.7159\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.985587\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.35417 to 1.34499, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "41496/41496 [==============================] - 17s 421us/step - loss: 0.8361 - acc: 0.7890 - val_loss: 1.3287 - val_acc: 0.7227\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.985878\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.34499 to 1.32871, saving model to best_model.hdf5\n",
            "Epoch 10/10\n",
            "41496/41496 [==============================] - 18s 439us/step - loss: 0.7221 - acc: 0.8146 - val_loss: 1.3894 - val_acc: 0.7266\n",
            "\n",
            " ROC-AUC - epoch: 10 - score: 0.984989\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.32871\n",
            "Saved model to disk\n",
            "5782/5782 [==============================] - 0s 58us/step\n",
            "### Model with seed: 2019  for fold no. 1 ####\n",
            "Train on 41567 samples, validate on 10466 samples\n",
            "Epoch 1/10\n",
            "41567/41567 [==============================] - 20s 476us/step - loss: 4.9729 - acc: 0.1295 - val_loss: 3.6609 - val_acc: 0.3100\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.850666\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.66089, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "41567/41567 [==============================] - 17s 420us/step - loss: 2.8551 - acc: 0.4162 - val_loss: 2.1656 - val_acc: 0.5268\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.964583\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.66089 to 2.16561, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "41567/41567 [==============================] - 17s 420us/step - loss: 1.9710 - acc: 0.5610 - val_loss: 1.7328 - val_acc: 0.6211\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.979178\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.16561 to 1.73275, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "41567/41567 [==============================] - 18s 421us/step - loss: 1.6056 - acc: 0.6306 - val_loss: 1.5347 - val_acc: 0.6613\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.982793\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.73275 to 1.53467, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "41567/41567 [==============================] - 18s 441us/step - loss: 1.3844 - acc: 0.6794 - val_loss: 1.4987 - val_acc: 0.6742\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.984388\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.53467 to 1.49869, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "41567/41567 [==============================] - 17s 420us/step - loss: 1.2450 - acc: 0.7067 - val_loss: 1.4183 - val_acc: 0.6905\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.984929\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.49869 to 1.41834, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "41567/41567 [==============================] - 17s 420us/step - loss: 1.1193 - acc: 0.7309 - val_loss: 1.3601 - val_acc: 0.7119\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.985501\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.41834 to 1.36007, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "41567/41567 [==============================] - 18s 423us/step - loss: 0.9676 - acc: 0.7613 - val_loss: 1.3710 - val_acc: 0.7087\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.985266\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.36007\n",
            "Epoch 9/10\n",
            "41567/41567 [==============================] - 18s 440us/step - loss: 0.8383 - acc: 0.7895 - val_loss: 1.3556 - val_acc: 0.7154\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.985292\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.36007 to 1.35560, saving model to best_model.hdf5\n",
            "Epoch 10/10\n",
            "41567/41567 [==============================] - 19s 460us/step - loss: 0.7260 - acc: 0.8123 - val_loss: 1.3943 - val_acc: 0.7191\n",
            "\n",
            " ROC-AUC - epoch: 10 - score: 0.984961\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.35560\n",
            "Saved model to disk\n",
            "5782/5782 [==============================] - 0s 58us/step\n",
            "### Model with seed: 2019  for fold no. 2 ####\n",
            "Train on 41618 samples, validate on 10415 samples\n",
            "Epoch 1/10\n",
            "41618/41618 [==============================] - 19s 453us/step - loss: 4.9713 - acc: 0.1297 - val_loss: 3.5817 - val_acc: 0.3158\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.873588\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.58166, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "41618/41618 [==============================] - 19s 449us/step - loss: 2.8576 - acc: 0.4146 - val_loss: 2.2672 - val_acc: 0.5095\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.962584\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.58166 to 2.26722, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "41618/41618 [==============================] - 18s 426us/step - loss: 2.0046 - acc: 0.5533 - val_loss: 1.7925 - val_acc: 0.6124\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.976732\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.26722 to 1.79250, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "41618/41618 [==============================] - 18s 440us/step - loss: 1.6218 - acc: 0.6321 - val_loss: 1.6316 - val_acc: 0.6395\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.980286\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.79250 to 1.63157, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "41618/41618 [==============================] - 18s 424us/step - loss: 1.4012 - acc: 0.6766 - val_loss: 1.5592 - val_acc: 0.6657\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.980611\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.63157 to 1.55924, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "41618/41618 [==============================] - 19s 451us/step - loss: 1.2537 - acc: 0.7058 - val_loss: 1.4734 - val_acc: 0.6835\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.982440\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.55924 to 1.47344, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "41618/41618 [==============================] - 18s 423us/step - loss: 1.1193 - acc: 0.7318 - val_loss: 1.4511 - val_acc: 0.6940\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.982379\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.47344 to 1.45112, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "41618/41618 [==============================] - 18s 424us/step - loss: 0.9759 - acc: 0.7615 - val_loss: 1.4474 - val_acc: 0.6984\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.982789\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.45112 to 1.44739, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "41618/41618 [==============================] - 18s 423us/step - loss: 0.8477 - acc: 0.7891 - val_loss: 1.4378 - val_acc: 0.7089\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.982620\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.44739 to 1.43778, saving model to best_model.hdf5\n",
            "Epoch 10/10\n",
            "41618/41618 [==============================] - 19s 449us/step - loss: 0.7271 - acc: 0.8155 - val_loss: 1.4861 - val_acc: 0.7118\n",
            "\n",
            " ROC-AUC - epoch: 10 - score: 0.982341\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.43778\n",
            "Saved model to disk\n",
            "5782/5782 [==============================] - 0s 56us/step\n",
            "### Model with seed: 2019  for fold no. 3 ####\n",
            "Train on 41684 samples, validate on 10349 samples\n",
            "Epoch 1/10\n",
            "41684/41684 [==============================] - 19s 449us/step - loss: 4.9254 - acc: 0.1372 - val_loss: 3.4925 - val_acc: 0.3312\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.874900\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.49255, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "41684/41684 [==============================] - 18s 423us/step - loss: 2.7718 - acc: 0.4262 - val_loss: 2.1399 - val_acc: 0.5390\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.965502\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.49255 to 2.13990, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "41684/41684 [==============================] - 18s 431us/step - loss: 1.9571 - acc: 0.5633 - val_loss: 1.7234 - val_acc: 0.6262\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.977748\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.13990 to 1.72342, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "41684/41684 [==============================] - 19s 449us/step - loss: 1.6020 - acc: 0.6335 - val_loss: 1.5624 - val_acc: 0.6537\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.981171\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.72342 to 1.56242, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "41684/41684 [==============================] - 18s 432us/step - loss: 1.3928 - acc: 0.6779 - val_loss: 1.5243 - val_acc: 0.6621\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.982206\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.56242 to 1.52430, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "41684/41684 [==============================] - 18s 423us/step - loss: 1.2505 - acc: 0.7056 - val_loss: 1.4141 - val_acc: 0.6933\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.982933\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.52430 to 1.41415, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "41684/41684 [==============================] - 18s 439us/step - loss: 1.1218 - acc: 0.7339 - val_loss: 1.3564 - val_acc: 0.7058\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.984065\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.41415 to 1.35644, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "41684/41684 [==============================] - 18s 433us/step - loss: 0.9637 - acc: 0.7641 - val_loss: 1.3459 - val_acc: 0.7140\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.983886\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.35644 to 1.34587, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "41684/41684 [==============================] - 18s 423us/step - loss: 0.8392 - acc: 0.7892 - val_loss: 1.3616 - val_acc: 0.7157\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.984260\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.34587\n",
            "Epoch 10/10\n",
            "41684/41684 [==============================] - 17s 418us/step - loss: 0.7282 - acc: 0.8117 - val_loss: 1.4060 - val_acc: 0.7161\n",
            "\n",
            " ROC-AUC - epoch: 10 - score: 0.983766\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.34587\n",
            "Saved model to disk\n",
            "5782/5782 [==============================] - 0s 59us/step\n",
            "### Model with seed: 2019  for fold no. 4 ####\n",
            "Train on 41767 samples, validate on 10266 samples\n",
            "Epoch 1/10\n",
            "41767/41767 [==============================] - 20s 476us/step - loss: 4.9261 - acc: 0.1348 - val_loss: 3.4956 - val_acc: 0.3369\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.878054\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.49557, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "41767/41767 [==============================] - 18s 423us/step - loss: 2.7653 - acc: 0.4257 - val_loss: 2.1950 - val_acc: 0.5205\n",
            "\n",
            " ROC-AUC - epoch: 2 - score: 0.968029\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.49557 to 2.19500, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "41767/41767 [==============================] - 18s 422us/step - loss: 1.9340 - acc: 0.5674 - val_loss: 1.7845 - val_acc: 0.6072\n",
            "\n",
            " ROC-AUC - epoch: 3 - score: 0.978041\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.19500 to 1.78450, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "41767/41767 [==============================] - 18s 421us/step - loss: 1.5884 - acc: 0.6348 - val_loss: 1.6033 - val_acc: 0.6485\n",
            "\n",
            " ROC-AUC - epoch: 4 - score: 0.981232\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.78450 to 1.60327, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "41767/41767 [==============================] - 19s 449us/step - loss: 1.3792 - acc: 0.6815 - val_loss: 1.5235 - val_acc: 0.6699\n",
            "\n",
            " ROC-AUC - epoch: 5 - score: 0.982922\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.60327 to 1.52348, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "41767/41767 [==============================] - 18s 421us/step - loss: 1.2321 - acc: 0.7091 - val_loss: 1.4432 - val_acc: 0.6927\n",
            "\n",
            " ROC-AUC - epoch: 6 - score: 0.983306\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.52348 to 1.44319, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "41767/41767 [==============================] - 18s 422us/step - loss: 1.1054 - acc: 0.7349 - val_loss: 1.4274 - val_acc: 0.6974\n",
            "\n",
            " ROC-AUC - epoch: 7 - score: 0.983918\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.44319 to 1.42739, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "41767/41767 [==============================] - 18s 421us/step - loss: 0.9498 - acc: 0.7661 - val_loss: 1.3896 - val_acc: 0.7091\n",
            "\n",
            " ROC-AUC - epoch: 8 - score: 0.984411\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.42739 to 1.38962, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "41767/41767 [==============================] - 20s 485us/step - loss: 0.8158 - acc: 0.7928 - val_loss: 1.3822 - val_acc: 0.7121\n",
            "\n",
            " ROC-AUC - epoch: 9 - score: 0.984450\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.38962 to 1.38224, saving model to best_model.hdf5\n",
            "Epoch 10/10\n",
            "41767/41767 [==============================] - 18s 421us/step - loss: 0.7086 - acc: 0.8161 - val_loss: 1.4318 - val_acc: 0.7141\n",
            "\n",
            " ROC-AUC - epoch: 10 - score: 0.984176\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.38224\n",
            "Saved model to disk\n",
            "5782/5782 [==============================] - 0s 57us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVCbgUPfKL5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test=pred_test*8/7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTzDAHadI8I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ROC scores on Test set for different codes\n",
        "scores=[]\n",
        "for i in range(396):\n",
        "  score = roc_auc_score(y_test[:,i], pred_test[:,i])\n",
        "  scores.append(score)\n",
        "  #print(\"score for index %s is %d\" (i,score))\n",
        "\n",
        "low = list(np.argsort(scores)[:5])\n",
        "high = list(np.argsort(scores)[::-1][:250])\n",
        "print('high scores is {}'.format(np.mean([scores[i] for i in high])))\n",
        "print('low scores is {}'.format(np.mean([scores[i] for i in low])))\n",
        "# ROC is not a problem in multi-class prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbDZj9NXtenR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving predictions for error analysis\n",
        "#os.mkdir('/content/drive/My Drive/CCP-ICDsearch/desc-icd9-modeldata_v2/M_maxlen7_350_icd/run3_embed_dpcnn_74.3/')\n",
        "os.chdir('/content/drive/My Drive/CCP-ICDsearch/desc-icd9-modeldata_v2/M_maxlen7_350_icd/run3_embed_dpcnn_74.3/')\n",
        "\n",
        "itoicd = dict((v,k) for k,v in icd_dict.items())\n",
        "test_pred = np.argmax(pred_test, axis=1)\n",
        "test_act = np.argmax(y_test, axis=1) \n",
        "test_pred_proba = np.max(pred_test,axis=1)\n",
        "\n",
        "mdf = pd.DataFrame({'test_act':[itoicd[i] for i in list(test_act)],'test_pred':[itoicd[i] for i in list(test_pred)], 'prob': list(test_pred_proba)})\n",
        "d = pd.DataFrame(pred_test)\n",
        "d.columns= [itoicd[i] for i in list(d.columns)]\n",
        "\n",
        "mdf = pd.concat([mdf,d],axis=1)\n",
        "mdf.to_csv('pred_test.csv',index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLKZhRMgEO9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mdf.to_csv('pred_test1.csv',index=None)\n",
        "from google.colab import files\n",
        "files.download('pred_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_weiorJZVhZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Top Codes with low accuracy on test set\n",
        "#pred_test\n",
        "acc = [len(mdf[(mdf['test_pred']==i) & (mdf['test_act']==i)])/len(mdf[mdf['test_act']==i]) for i in mdf['test_act'].values]\n",
        "mdf2 = pd.DataFrame({'class_index': mdf['test_act'].values,'accuracy':acc})\n",
        "#print(len(mdf2[mdf2['accuracy']<0.5].class_index.unique().tolist()))\n",
        "codes= mdf2[mdf2['accuracy']<0.5].class_index.unique().tolist()\n",
        "mdf2.to_csv('error_codes.csv', index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsS4MwoqOvLK",
        "colab_type": "code",
        "outputId": "6cefc6db-c5ff-4a27-9f17-903746ebf455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Accuracy on various condifence levels and sample coverage on Test set\n",
        "#del CUTOFF\n",
        "for CUTOFF in range(0,100,5):\n",
        "    acc = len(mdf[(mdf['prob']>CUTOFF/100) & (mdf['test_act']==mdf['test_pred'])])/len(mdf[(mdf['prob']>CUTOFF/100)])\n",
        "    cov = len(mdf[(mdf['prob']>CUTOFF/100)])/len(mdf)\n",
        "    print(\"Confidence: %f ## Accuracy: %f ## Coverage on Test dataset: %f\" % (CUTOFF/100,acc,cov))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confidence: 0.000000 ## Accuracy: 0.741439 ## Coverage on Test dataset: 1.000000\n",
            "Confidence: 0.050000 ## Accuracy: 0.754312 ## Coverage on Test dataset: 0.982705\n",
            "Confidence: 0.100000 ## Accuracy: 0.760306 ## Coverage on Test dataset: 0.973366\n",
            "Confidence: 0.150000 ## Accuracy: 0.770217 ## Coverage on Test dataset: 0.958146\n",
            "Confidence: 0.200000 ## Accuracy: 0.778305 ## Coverage on Test dataset: 0.947077\n",
            "Confidence: 0.250000 ## Accuracy: 0.790347 ## Coverage on Test dataset: 0.928053\n",
            "Confidence: 0.300000 ## Accuracy: 0.800304 ## Coverage on Test dataset: 0.910239\n",
            "Confidence: 0.350000 ## Accuracy: 0.811079 ## Coverage on Test dataset: 0.889831\n",
            "Confidence: 0.400000 ## Accuracy: 0.823494 ## Coverage on Test dataset: 0.864234\n",
            "Confidence: 0.450000 ## Accuracy: 0.838403 ## Coverage on Test dataset: 0.835870\n",
            "Confidence: 0.500000 ## Accuracy: 0.852363 ## Coverage on Test dataset: 0.801280\n",
            "Confidence: 0.550000 ## Accuracy: 0.868486 ## Coverage on Test dataset: 0.766690\n",
            "Confidence: 0.600000 ## Accuracy: 0.882409 ## Coverage on Test dataset: 0.732446\n",
            "Confidence: 0.650000 ## Accuracy: 0.894491 ## Coverage on Test dataset: 0.703217\n",
            "Confidence: 0.700000 ## Accuracy: 0.905943 ## Coverage on Test dataset: 0.669319\n",
            "Confidence: 0.750000 ## Accuracy: 0.915143 ## Coverage on Test dataset: 0.633864\n",
            "Confidence: 0.800000 ## Accuracy: 0.924132 ## Coverage on Test dataset: 0.592701\n",
            "Confidence: 0.850000 ## Accuracy: 0.935514 ## Coverage on Test dataset: 0.549810\n",
            "Confidence: 0.900000 ## Accuracy: 0.947130 ## Coverage on Test dataset: 0.497233\n",
            "Confidence: 0.950000 ## Accuracy: 0.959810 ## Coverage on Test dataset: 0.400208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drw28VPA65xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVM2EqgpX0Lr",
        "colab_type": "code",
        "outputId": "a114457a-b618-4894-cd1d-3de58bc6d817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Steps- for re generating the Data for modeling\n",
        "\n",
        "#datasize for icd codes\n",
        "\n",
        "strain = np.argmax(np.vstack([y_train,y_test]), axis=1)\n",
        "codesize = [sum(strain==icd_dict[i]) for i in codes]\n",
        "#print(len([i for i in codesize if i < 30]))\n",
        "codesize2 = [sum(strain==icd_dict[i]) for i in (set([itoicd[i] for i in set(strain)])-set(codes))]\n",
        "#print(len([i for i in codesize2 if i < 30]))\n",
        "\n",
        "print(\"mean dataset size of error codes is %d\" % (np.mean(codesize)))\n",
        "print(\"mean  dataset size of non-error codes is %d\" % (np.mean(codesize2)))\n",
        "print(\"median dataset size of error codes is %d\" % (np.median(codesize)))\n",
        "print(\"median  dataset size of non-error codes is %d\" % (np.median(codesize2)))\n",
        "\n",
        "# word entropy for icd codes\n",
        "\n",
        "idx = [[j for j in range(len(strain)) if strain[j]==icd_dict[i]] for i in codes]\n",
        "idx2 = [[j for j in range(len(strain)) if strain[j]==icd_dict[i]] for i in (set([itoicd[i] for i in set(strain)])-set(codes))]\n",
        "x = np.vstack([x_train, x_test])\n",
        "word_ent = [len(set([j for i in [[list(x[i]) for i in p] for p in idx][p] for j in i])) for p in range(len([[list(x[i]) for i in p] for p in idx]))]\n",
        "word_ent2 = [len(set([j for i in [[list(x[i]) for i in p] for p in idx2][p] for j in i])) for p in range(len([[list(x[i]) for i in p] for p in idx2]))]\n",
        "\n",
        "print(\"#################################################\")\n",
        "print(\"mean # of unqiue words in error codes is %d\" % (np.mean(word_ent)))\n",
        "print(\"mean # of unqiue words in non-error codes is %d\" % (np.mean(word_ent2)))\n",
        "print(\"median # of unqiue words in error codes is %d\" % (np.median(word_ent)))\n",
        "print(\"median # of unqiue words in non-error codes is %d\" % (np.median(word_ent2)))\n",
        "\n",
        "\n",
        "#codesize\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean dataset size of error codes is 70\n",
            "mean  dataset size of non-error codes is 193\n",
            "median dataset size of error codes is 50\n",
            "median  dataset size of non-error codes is 104\n",
            "#################################################\n",
            "mean # of unqiue words in error codes is 76\n",
            "mean # of unqiue words in non-error codes is 109\n",
            "median # of unqiue words in error codes is 58\n",
            "median # of unqiue words in non-error codes is 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj0mhWJ0Qg2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('model_v1.0.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model_v1.0.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=[\"accuracy\"])\n",
        "score = np.argmax(loaded_model.predict(DATASET, batch_size = 16, verbose = 1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}